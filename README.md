# üï∑Ô∏è WebReconSpider

**WebReconSpider** √© uma ferramenta de reconhecimento web (recon) baseada em Python e **Scrapy**. Este script atua como um *crawler* que navega por um site alvo para extrair informa√ß√µes valiosas, mapear ativos e identificar vetores de interesse, salvando tudo em um arquivo JSON estruturado.

Ele inclui um **Middleware Personalizado** para lidar corretamente com dom√≠nios que utilizam portas n√£o padr√£o (ex: `http://exemplo.com:8080`).

## Funcionalidades

O script varre o site inicial e segue links internos recursivamente, coletando os seguintes dados:

  * ** E-mails:** Extrai endere√ßos de e-mail encontrados no texto.
  * ** Links:** Mapeia links internos e externos.
  * ** Arquivos Externos:** Identifica links para documentos (PDF, DOCX, XLSX) e folhas de estilo (CSS).
  * ** Arquivos JS:** Lista todos os scripts JavaScript (`.js`) importados.
  * ** Campos de Formul√°rio:** Coleta os nomes (`name`) de inputs, textareas e selects (√∫til para mapear superf√≠cies de ataque).
  * ** M√≠dias:** Extrai URLs de Imagens, V√≠deos e √Åudio.
  * ** Coment√°rios HTML:** Captura coment√°rios ocultos no c√≥digo fonte (que muitas vezes cont√™m informa√ß√µes sens√≠veis ou notas de desenvolvedores).

##  Pr√©-requisitos

Voc√™ precisa ter o **Python 3.6+** instalado. As depend√™ncias do projeto s√£o m√≠nimas.

### Instala√ß√£o

1.  Clone este reposit√≥rio ou salve o script.
2.  Instale o framework Scrapy:

<!-- end list -->

```bash
pip install scrapy
```

##  Como Usar

Execute o script via linha de comando passando a URL alvo como argumento.

**Sintaxe:**

```bash
python nome_do_script.py <URL_ALVO>
```

**Exemplo:**

```bash
python ReconSpider.py https://www.exemplo.com.br
```

> **Nota:** O crawler est√° configurado para se restringir ao dom√≠nio da URL inicial. Ele n√£o seguir√° links para sites externos (apenas registrar√° que o link existe).

##  Sa√≠da (Output)

Ap√≥s a conclus√£o da varredura, o script gerar√° um arquivo chamado `results.json` no mesmo diret√≥rio.

Exemplo da estrutura do JSON gerado:

```json
{
    "emails": ["contato@exemplo.com", "suporte@exemplo.com"],
    "links": ["https://www.exemplo.com/sobre", "https://google.com"],
    "external_files": ["https://www.exemplo.com/estilo.css", "https://www.exemplo.com/manual.pdf"],
    "js_files": ["https://www.exemplo.com/app.js", "https://code.jquery.com/jquery.min.js"],
    "form_fields": ["usuario", "senha", "email_newsletter"],
    "images": ["https://www.exemplo.com/logo.png"],
    "videos": [],
    "audio": [],
    "comments": [""]
}
```

##  Detalhes T√©cnicos

  * **CustomOffsiteMiddleware:** O script substitui o middleware padr√£o do Scrapy para permitir o rastreamento em URLs que especificam portas (ex: `localhost:8000`), algo que a configura√ß√£o padr√£o muitas vezes bloqueia.
  * **Crawling Recursivo:** Ele segue apenas links que pertencem ao mesmo dom√≠nio (`netloc`) da URL inicial para evitar sair do escopo.
